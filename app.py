import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import tempfile
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .stChat message {
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .main-header {
        text-align: center;
        padding: 1rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .sidebar-info {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# API í‚¤ ì„¤ì •
GOOGLE_API_KEY = st.secrets["GEMINI_API_KEY"]

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
CUSTOM_PROMPT = PromptTemplate(
    template="""ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€í•  ë•Œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
3. ì¶”ì¸¡í•˜ê±°ë‚˜ ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
4. ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
5. ê°€ëŠ¥í•˜ë‹¤ë©´ ë¬¸ì„œì˜ ì–´ëŠ ë¶€ë¶„ì„ ì°¸ì¡°í–ˆëŠ”ì§€ ì–¸ê¸‰í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ì§ˆë¬¸: {question}

ë‹µë³€:""",
    input_variables=["context", "chat_history", "question"]
)


@st.cache_resource
def initialize_llm():
    """LLM ì´ˆê¸°í™”"""
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3,
        convert_system_message_to_human=True
    )


@st.cache_resource
def initialize_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=GOOGLE_API_KEY
    )


def load_and_process_pdf(pdf_file):
    """PDF íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.getvalue())
        tmp_path = tmp_file.name

    try:
        # PDF ë¡œë“œ
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        splits = text_splitter.split_documents(documents)

        return splits
    finally:
        os.unlink(tmp_path)


def create_vectorstore(documents):
    """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    embeddings = initialize_embeddings()
    vectorstore = FAISS.from_documents(documents, embeddings)
    return vectorstore


def create_conversation_chain(vectorstore):
    """ëŒ€í™” ì²´ì¸ ìƒì„±"""
    llm = initialize_llm()

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": CUSTOM_PROMPT},
        return_source_documents=True,
        verbose=False
    )

    return chain


def main():
    # í—¤ë”
    st.markdown('''<div class="main-header"><h1>ğŸ“š PDF RAG ì±—ë´‡</h1><p>PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”!</p></div>''', unsafe_allow_html=True)

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")

        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=["pdf"],
            help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."
        )

        # ê¸°ë³¸ test.pdf ì‚¬ìš© ì˜µì…˜
        use_default = st.checkbox("ê¸°ë³¸ test.pdf ì‚¬ìš©", value=False)

        if use_default and os.path.exists("test.pdf"):
            with open("test.pdf", "rb") as f:
                uploaded_file = f
                st.success("âœ… test.pdf ë¡œë“œë¨")

        st.markdown('''<div class="sidebar-info">''', unsafe_allow_html=True)
        st.markdown("""
        ### ì‚¬ìš© ë°©ë²•
        1. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤
        2. ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤
        3. ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤
        4. AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
        """)
        st.markdown('''</div>''', unsafe_allow_html=True)

        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chain = None
            st.rerun()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chain" not in st.session_state:
        st.session_state.chain = None
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None

    # PDF ì²˜ë¦¬
    if uploaded_file is not None:
        if st.session_state.vectorstore is None:
            with st.spinner("ğŸ“– PDF ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # PDF ë¡œë“œ ë° ì²˜ë¦¬
                    if hasattr(uploaded_file, 'getvalue'):
                        documents = load_and_process_pdf(uploaded_file)
                    else:
                        loader = PyPDFLoader("test.pdf")
                        documents = loader.load()
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=1000,
                            chunk_overlap=200
                        )
                        documents = text_splitter.split_documents(documents)

                    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                    st.session_state.vectorstore = create_vectorstore(documents)

                    # ëŒ€í™” ì²´ì¸ ìƒì„±
                    st.session_state.chain = create_conversation_chain(
                        st.session_state.vectorstore
                    )

                    st.success(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ({len(documents)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨)")

                except Exception as e:
                    st.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    return

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    chat_container = st.container()

    with chat_container:
        # ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"):
                st.markdown(message["content"])
                if "sources" in message and message["sources"]:
                    with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**ì¶œì²˜ {i}** (í˜ì´ì§€ {source.get('page', 'N/A')})")
                            st.caption(source.get('content', '')[:300] + "...")

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("PDF ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...", disabled=st.session_state.chain is None):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            with st.spinner("ìƒê° ì¤‘..."):
                try:
                    response = st.session_state.chain.invoke({"question": prompt})
                    answer = response["answer"]

                    # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
                    sources = []
                    if "source_documents" in response:
                        for doc in response["source_documents"]:
                            sources.append({
                                "page": doc.metadata.get("page", "N/A"),
                                "content": doc.page_content
                            })

                    st.markdown(answer)

                    # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                    if sources:
                        with st.expander("ğŸ“ ì°¸ì¡° ë¬¸ì„œ"):
                            for i, source in enumerate(sources, 1):
                                st.markdown(f"**ì¶œì²˜ {i}** (í˜ì´ì§€ {source['page']})")
                                st.caption(source['content'][:300] + "...")

                    # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources
                    })

                except Exception as e:
                    error_msg = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })

    # PDF ë¯¸ì—…ë¡œë“œ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€
    if st.session_state.chain is None:
        st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
